{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos lineales\n",
    "Los modelos lineales son útiles cuando disponemos de pocos datos para espacios de características muy grandes, como en clasificación de textos. Además, son una buena forma de estudiar los métodos de regularización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos lineales para regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los modelos lineales aprenden un vector que contienen los coeficientes, ``coef_``, y una ordenada en el origen, ``intercept_``, que les permite hacer predicciones usando una combinación lineal de las variables de entrada:\n",
    "\n",
    "```\n",
    "y_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_\n",
    "```\n",
    "\n",
    "La diferencia entren los modelos lineales para regresión es que tienen restricciones o penalizaciones en el vector ``coef_`` como forma de regularización, además de intentar aproximar bien los datos de entrada.\n",
    "El modelo lineal más estándar es la regresión de mínimos cuadrados ('*ordinary least squares regression*'), a menudo conocida como regresión lineal. No pone ninguna restricción en ``coef_``, por lo que, cuando el número de características es muy alto, no funciona correctamente y puede resultar en sobre-aprendizaje.\n",
    "\n",
    "Vamos a hacer una simulación simple para ver el comportamiento de estos modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y, true_coefficient = make_regression(n_samples=200, n_features=30, n_informative=10, noise=100, coef=True, random_state=5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, train_size=60, test_size=140)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal\n",
    "\n",
    "$$ \\text{min}_{\\mathbf{w}, b} \\sum_i || \\mathbf{w}^\\mathsf{T}\\mathbf{x}_i + b  - y_i||^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_regression = LinearRegression().fit(X_train, y_train)\n",
    "print(\"R^2 en entrenamiento: %f\" % linear_regression.score(X_train, y_train))\n",
    "print(\"R^2 en test: %f\" % linear_regression.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(np.dot(X, true_coefficient), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "coefficient_sorting = np.argsort(true_coefficient)[::-1] # lista[::-1] sirve para darle la vuelta a lista\n",
    "plt.plot(true_coefficient[coefficient_sorting], \"o\", label=\"verdadero\")\n",
    "plt.plot(linear_regression.coef_[coefficient_sorting], \"o\", label=\"regresión lineal\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(est, X, y):\n",
    "    training_set_size, train_scores, test_scores = learning_curve(est, X, y, train_sizes=np.linspace(.1, 1, 20))\n",
    "    estimator_name = est.__class__.__name__\n",
    "    line = plt.plot(training_set_size, train_scores.mean(axis=1), '--', label=\"puntuaciones de entrenamiento \" + estimator_name)\n",
    "    plt.plot(training_set_size, test_scores.mean(axis=1), '-', label=\"puntuaciones de test \" + estimator_name, c=line[0].get_color())\n",
    "    plt.xlabel('Tamaño del conjunto de entrenamiento')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.figure()    \n",
    "plot_learning_curve(LinearRegression(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión de cresta (*Ridge Regression*, penalización L2)\n",
    "\n",
    "**El estimador de cresta (``Ridge``)** es una regularización simple (llamada regularización L2) para el modelo LinearRegression. En particular, tiene el beneficio de no ser más costoso computacionalmente que la estimación basada en mínimos cuadrados.\n",
    "\n",
    "$$ \\text{min}_{\\mathbf{w},b}  \\sum_i || \\mathbf{w}^\\mathsf{T}\\mathbf{x}_i + b  - y_i||^2  + \\alpha ||\\mathbf{w}||_2^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cantidad de regularización se ajusta a través del parámetro `alpha` del modelo Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_models = {}\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for alpha in [100, 10, 1, .01]:\n",
    "    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    training_scores.append(ridge.score(X_train, y_train))\n",
    "    test_scores.append(ridge.score(X_test, y_test))\n",
    "    ridge_models[alpha] = ridge\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_scores, label=\"puntuaciones de entrenamiento\")\n",
    "plt.plot(test_scores, label=\"puntuaciones de test\")\n",
    "plt.xticks(range(4), [100, 10, 1, .01])\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(true_coefficient[coefficient_sorting], \"o\", label=\"true\", c='b')\n",
    "\n",
    "for i, alpha in enumerate([100, 10, 1, .01]):\n",
    "    plt.plot(ridge_models[alpha].coef_[coefficient_sorting], \"o\", label=\"alpha = %.2f\" % alpha, c=plt.cm.summer(i / 3.))\n",
    "    \n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustar `alpha` es muy importante para el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_learning_curve(LinearRegression(), X, y)\n",
    "plot_learning_curve(Ridge(alpha=10), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso (penalización L1)\n",
    "**El estimador ``Lasso``** es útil para conseguir imponer dispersión en los coeficientes. En otras palabras, se debería preferir esta penalización si creemos que muchas de las características no son relevantes. Se consigue a través de la regularización L1.\n",
    "\n",
    "$$ \\text{min}_{\\mathbf{w}, b} \\sum_i \\frac{1}{2} || \\mathbf{w}^\\mathsf{T}\\mathbf{x}_i + b  - y_i||^2  + \\alpha ||\\mathbf{w}||_1$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_models = {}\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for alpha in [30, 10, 1, .01]:\n",
    "    lasso = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "    training_scores.append(lasso.score(X_train, y_train))\n",
    "    test_scores.append(lasso.score(X_test, y_test))\n",
    "    lasso_models[alpha] = lasso\n",
    "plt.figure()\n",
    "plt.plot(training_scores, label=\"puntuaciones de entrenamiento\")\n",
    "plt.plot(test_scores, label=\"puntuaciones de test\")\n",
    "plt.xticks(range(4), [30, 10, 1, .01])\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(true_coefficient[coefficient_sorting], \"o\", label=\"true\", c='b')\n",
    "\n",
    "for i, alpha in enumerate([30, 10, 1, .01]):\n",
    "    plt.plot(lasso_models[alpha].coef_[coefficient_sorting], \"o\", label=\"alpha = %.2f\" % alpha, c=plt.cm.summer(i / 3.))\n",
    "    \n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_learning_curve(LinearRegression(), X, y)\n",
    "plot_learning_curve(Ridge(alpha=10), X, y)\n",
    "plot_learning_curve(Lasso(alpha=10), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de escoger ``Ridge`` **o** ``Lasso``, puedes utilizar [ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html), que considera ambas formas de regularización y proporciona un parámetro para sopesarlas. ``ElasticNet`` suele comportarse como el mejor de los dos modelos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos lineales para clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los modelos lineales para clasificación aprenden un vector de coeficientes ``coef_`` y un valor de umbral ``intercept_`` para hacer predicciones mediante una combinación lineal de las características:\n",
    "```\n",
    "y_pred = x_test[0] * coef_[0] + ... + x_test[n_features-1] * coef_[n_features-1] + intercept_ > 0\n",
    "```\n",
    "Como puedes ver, es muy similar a regresión.\n",
    "\n",
    "De nuevo, la diferencia de los modelos lineales para clasificación es el tipo de regularización que imponen en ``coef_`` e ``intercept_``, para hay también diferencias menores en como se mide la bondad de ajuste con respecto al conjunto de entrenamiento (lo que se conoce también como función de pérdida).\n",
    "\n",
    "Los dos modelos más comunes para clasificación lineal son la SVM lineal, implementada como ``LinearSVC`` y la regresión logística, ``LogisticRegression``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La influencia de C en ``LinearSVC``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `C` de ``LinearSVC`` controla la regularización que sufre el modelo.\n",
    "\n",
    "Valores bajos de `C` llevan a más regularización y a modelos más simples (permitiendo más errores), mientras que valores altos de `C` llevan a menos regularización y a modelos más forzados a intentar clasificar todo correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from figures import plot_linear_svc_regularization\n",
    "plot_linear_svc_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar a la distinción Ridge/Lasso, puedes ajustar el parámetro `penalty` a 'l1' para forzar dispersión en los coeficientes (similar a Lasso) o 'l2' para forzar coeficientes más pequeños (similar a Ridge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación multi-clase lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "plt.figure()\n",
    "X, y = make_blobs(random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=plt.cm.spectral(y / 2.));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "print(linear_svm.coef_.shape)\n",
    "print(linear_svm.intercept_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=plt.cm.spectral(y / 2.))\n",
    "line = np.linspace(-15, 15)\n",
    "for coef, intercept in zip(linear_svm.coef_, linear_svm.intercept_):\n",
    "    plt.plot(line, -(line * coef[0] + intercept) / coef[1])\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlim(-10, 8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los puntos se clasifican utilizando la estrategia `one-vs-rest` (también llamada `one-vs-all`), con la que asignamos un patrón de test a la clase cuyo modelo tiene más máxima certeza de clasificación (en el caso de la SVM, la máxima distancia con respecto al hiperplano separador)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EJERCICIO</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Utiliza ``LogisticRegression`` para clasificar el dataset de dígitos manuscritos y aplica búsqueda *grid* para ajustar el parámetro `C`.\n",
    "      </li>\n",
    "      <li>\n",
    "      ¿Cómo piensas que cambiarán las curvas de aprendizaje anteriores cuando incrementes o decrementes `alpha`?\n",
    "Prueba a hacerlo en el código y ver si tu intuición es correcta.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "# Divide el dataset y realiza la búsqueda grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
